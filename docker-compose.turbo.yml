# Ultra-Optimized Docker Compose for Unified PIM MCP Development
# Designed for maximum performance and fastest startup times
version: '3.8'

x-common-healthcheck: &common-healthcheck
  interval: 10s
  timeout: 3s
  retries: 2
  start_period: 5s

x-common-restart: &common-restart
  restart_policy:
    condition: unless-stopped
    delay: 2s
    max_attempts: 3

x-common-logging: &common-logging
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"

services:
  # ChromaDB - Optimized for development speed
  chromadb-turbo:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    tmpfs:
      - /tmp
    shm_size: 1gb
    volumes:
      - chromadb_turbo_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
      - ALLOW_RESET=true
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]
      - CHROMA_LOG_LEVEL=INFO
      # Performance optimizations
      - CHROMA_DB_IMPL=duckdb+parquet
      - CHROMA_SEGMENT_CACHE_POLICY=LRU
      - CHROMA_MAX_BATCH_SIZE=5000
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "curl", "-f", "--max-time", "2", "http://localhost:8000/api/v1/heartbeat"]
    <<: *common-restart
    <<: *common-logging
    container_name: pim-chromadb-turbo
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  # Redis - Ultra-fast caching configuration
  redis-turbo:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    tmpfs:
      - /tmp
    volumes:
      - redis_turbo_data:/data
    command: >
      redis-server
      --save 60 1000
      --appendonly yes
      --appendfsync everysec
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 300
      --timeout 0
      --tcp-backlog 511
      --databases 16
      --loglevel notice
      --syslog-enabled no
      --stop-writes-on-bgsave-error no
      --rdbcompression yes
      --rdbchecksum yes
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "redis-cli", "--latency-history", "-i", "1", "-c", "1", "ping"]
    <<: *common-restart
    <<: *common-logging
    container_name: pim-redis-turbo
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  # Lightweight PostgreSQL for metadata
  postgres-turbo:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    tmpfs:
      - /tmp
      - /var/run/postgresql
    volumes:
      - postgres_turbo_data:/var/lib/postgresql/data
      - ./scripts/sql/init-dev-db.sql:/docker-entrypoint-initdb.d/init-dev-db.sql
    environment:
      - POSTGRES_DB=unified_pim_turbo
      - POSTGRES_USER=turbo_user
      - POSTGRES_PASSWORD=turbo_dev_pass
      - POSTGRES_INITDB_ARGS=--auth-host=trust
      # Performance tuning
      - POSTGRES_SHARED_BUFFERS=256MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=1GB
      - POSTGRES_MAINTENANCE_WORK_MEM=64MB
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_WAL_BUFFERS=16MB
      - POSTGRES_DEFAULT_STATISTICS_TARGET=100
    command: >
      postgres
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=8
      -c max_parallel_workers_per_gather=4
      -c max_parallel_workers=8
      -c max_parallel_maintenance_workers=4
      -c synchronous_commit=off
      -c fsync=off
      -c full_page_writes=off
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD-SHELL", "pg_isready -U turbo_user -d unified_pim_turbo"]
    <<: *common-restart
    <<: *common-logging
    container_name: pim-postgres-turbo
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'

  # High-performance MockServer for OAuth testing
  mock-oauth-turbo:
    image: mockserver/mockserver:5.15.0
    ports:
      - "1080:1080"
    tmpfs:
      - /tmp
    environment:
      - MOCKSERVER_LOG_LEVEL=WARN
      - MOCKSERVER_PROPERTY_FILE=/config/mockserver.properties
      - MOCKSERVER_INITIALIZATION_JSON_PATH=/config/mock-oauth-init.json
      - JVM_OPTIONS=-Xms128m -Xmx256m -XX:+UseG1GC -XX:+UseStringDeduplication
    volumes:
      - ./dev-tools/mock-oauth:/config:ro
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "curl", "-f", "--max-time", "2", "http://localhost:1080/mockserver/status"]
    <<: *common-restart
    <<: *common-logging
    container_name: pim-mock-oauth-turbo
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'

  # Ultra-fast MailHog for email testing
  mailhog-turbo:
    image: mailhog/mailhog:v1.0.1
    ports:
      - "1025:1025"
      - "8025:8025"
    tmpfs:
      - /tmp
    environment:
      - MH_STORAGE=memory
      - MH_HOSTNAME=localhost
      - MH_UI_WEB_PATH=mailhog
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "nc", "-z", "-v", "-w1", "localhost", "1025"]
    <<: *common-restart
    <<: *common-logging
    container_name: pim-mailhog-turbo
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'

  # Performance monitoring with Prometheus (optional)
  prometheus:
    image: prom/prometheus:v2.40.7
    ports:
      - "9090:9090"
    volumes:
      - ./dev-tools/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=1h'
      - '--storage.tsdb.retention.size=1GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
    <<: *common-restart
    <<: *common-logging
    container_name: pim-prometheus-turbo
    profiles: ["monitoring", "performance"]
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  # Grafana for performance dashboards (optional)
  grafana:
    image: grafana/grafana:9.3.2
    ports:
      - "3000:3000"
    tmpfs:
      - /tmp
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000/
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./dev-tools/monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./dev-tools/monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
    <<: *common-restart
    <<: *common-logging
    container_name: pim-grafana-turbo
    profiles: ["monitoring", "performance"]
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  # Node Exporter for system metrics (optional)
  node-exporter:
    image: prom/node-exporter:v1.5.0
    ports:
      - "9100:9100"
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    healthcheck:
      <<: *common-healthcheck
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9100/metrics"]
    <<: *common-restart
    <<: *common-logging
    container_name: pim-node-exporter-turbo
    profiles: ["monitoring", "performance"]
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'

volumes:
  chromadb_turbo_data:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=2g,uid=1000
  redis_turbo_data:
    driver: local
  postgres_turbo_data:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=1g,uid=999
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  default:
    name: pim-turbo-network
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 1500
    ipam:
      config:
        - subnet: 172.20.0.0/16